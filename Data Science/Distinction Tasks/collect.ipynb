{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6139240-3b34-461a-80ef-4320364f26db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A. Imports\n",
    "You need to import the following libraries. Install the libraries using \"conda install ... or pip install ...\" if they have not been installed on your machine. For example you can install google api python client by executing \"conda install google-api-python-client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14131785-ec21-4364-96fc-7d930999ba5f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google-api-python-client module not found. Installing...\n",
      "Requirement already satisfied: google-api-python-client in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.166.0)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client) (2.24.2)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.69.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (6.30.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
      "All required modules are installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1028)>\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "# The following lines should install all libraries you need - you can install the libraries manually if the script did not work \n",
    "required_modules = ['pandas', 'seaborn', 'matplotlib', 'google-api-python-client', 'datetime', 'configparser', 'nltk', 'langdetect', 'textblob', 'prettytable', 'tabulate', 'numpy']\n",
    "for module in required_modules:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "    except ImportError:\n",
    "        print(f\"{module} module not found. Installing...\")\n",
    "        subprocess.check_call(['pip', 'install', module])\n",
    "\n",
    "print(\"All required modules are installed.\")\n",
    "\n",
    "# import the installed libraries ...\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from datetime import datetime\n",
    "import os\n",
    "from configparser import ConfigParser\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "import langdetect\n",
    "from textblob import TextBlob\n",
    "import calendar\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter\n",
    "from prettytable import PrettyTable\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62382a9-a4eb-4079-88c7-86ff18f981ba",
   "metadata": {},
   "source": [
    "### B. Settings\n",
    "This section specifies the settings for connecting to the YouTube API and collecting the data about YouTube videos and their corresponding comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3914f598-971c-499a-9909-6776720a2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIDEOS_FILE = \"videos.csv\"\n",
    "COMMENTS_FILE = \"comments.csv\"\n",
    "CREDENTIALS_FILE = 'credentials.ini'\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2023, 1, 1)\n",
    "KEYWORDS =['coronavirus', 'covid', 'covid-19', 'pandemic']\n",
    "# You can use functin get_channel_info() to extract the channel ID of a sample video from a news publisher ...\n",
    "CHANNELS = {\n",
    "    'UCXIJgqnII2ZOINSWNOGFThA' : 'Fox News',\n",
    "    'UC16niRr50-MSBwiO3YDb3RA' : 'BBC News',\n",
    "    'UCupvZG-5ko_eiXAupbDfxWw' : 'CNN',\n",
    "    'UCaXkIU1QidjPwiAYu6GcHjg' : 'MSNBC'\n",
    "}\n",
    "MAX_VIDEOS = 50 # the maximum number of video that should be returned for each request. Acceptable values are 0 to 50\n",
    "QUERY= f\"intitle:{','.join(KEYWORDS)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b010ce18-ccee-459e-97bd-73d13e5ad0ec",
   "metadata": {},
   "source": [
    "### C. Load the credentials for authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea3dfcd6-0ae1-41bc-82c0-f9368d071e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_credentials():\n",
    "    try:\n",
    "        config = ConfigParser(interpolation=None)\n",
    "        config.read(CREDENTIALS_FILE)\n",
    "        developer_key = config.get('credentials_youtube', 'developer_key', fallback=None)\n",
    "        service_name = config.get('credentials_youtube', 'youtube_api_service_name', fallback=None)\n",
    "        service_version = config.get('credentials_youtube', 'youtube_api_version', fallback=None)\n",
    "        if not developer_key or not service_name or not service_version:\n",
    "            raise ValueError(\"Invalid credentials file\")\n",
    "\n",
    "        return {\n",
    "            'developer_key' : developer_key,\n",
    "            'service_name' : service_name,\n",
    "            'service_version' : service_version\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise ValueError(\"Failed to load credentials: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03189f46-940d-4822-976e-691e9b0cf503",
   "metadata": {
    "tags": []
   },
   "source": [
    "### D. Extract the channel_id and channel_title of a sample video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8503383a-2975-4339-a921-e8b547735f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function takes a video ID and a YouTube Object and returns the video's channel ID\n",
    "# See Section H (call the functions) to learn how to use this function\n",
    "def get_channel_info(video_id, youtube):\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet\",\n",
    "        id=video_id\n",
    "    )\n",
    "    response = request.execute()\n",
    "    channel_id = response['items'][0]['snippet']['channelId']\n",
    "    channel_title = response['items'][0]['snippet']['channelTitle']\n",
    "    return channel_id, channel_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978f3ff2-52f2-4395-8e08-bf4a7c4288e3",
   "metadata": {},
   "source": [
    "### E. Search for the videos from the channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7669b972-7e4c-4248-b705-0c1d2123ca59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_channel_videos(channel_ids, start_date, end_date, query, video_categories={}, max_videos=10):\n",
    "    df_list = []\n",
    "    for channel_id in channel_ids: \n",
    "        print(f\"-> collecting videos for channel: {CHANNELS[channel_id]}\")\n",
    "        try:\n",
    "            request = youtube.search().list(\n",
    "                part=\"snippet\",\n",
    "                type='video',\n",
    "                channelId=channel_id,\n",
    "                maxResults=max_videos, # specifies the maximum number of items that should be returned in the result set. Acceptable values are 0 to 50, inclusive.\n",
    "                q=query,\n",
    "                publishedAfter=start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                publishedBefore=end_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            )\n",
    "            response = request.execute()\n",
    "            videos = response['items']\n",
    "            data = []\n",
    "            for video in videos:\n",
    "                video_id = video['id']['videoId']\n",
    "                video_details = youtube.videos().list(\n",
    "                    part=\"snippet,statistics,contentDetails\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "                video_data = {\n",
    "                    'video_id' : video_id,\n",
    "                    'channel_id' : channel_id,\n",
    "                    'video_title': video_details['items'][0]['snippet']['title'],\n",
    "                    'channel_title': video_details['items'][0]['snippet']['channelTitle'],\n",
    "                    'category_name': video_categories.get(str(video_details['items'][0]['snippet']['categoryId']), 'Unknown'),\n",
    "                    'live_upcoming_none' : video_details['items'][0]['snippet']['liveBroadcastContent'],\n",
    "                    'view_count': video_details['items'][0]['statistics'].get('viewCount', 0),\n",
    "                    'like_count': video_details['items'][0]['statistics'].get('likeCount', 0),\n",
    "                    'dislike_count': video_details['items'][0]['statistics'].get('dislikeCount', 0),\n",
    "                    'comment_count': video_details['items'][0]['statistics'].get('commentCount', 0),\n",
    "                    'published_at': video_details['items'][0]['snippet']['publishedAt'],\n",
    "                    'tags': ','.join(video_details['items'][0]['snippet'].get('tags', [])),\n",
    "                    'duration': video_details['items'][0]['contentDetails'].get('duration', ''),\n",
    "                    'definition': video_details['items'][0]['contentDetails'].get('definition', 'unknown'),\n",
    "                    'caption': video_details['items'][0]['contentDetails'].get('caption', 'false'),\n",
    "                    'thumbnail' : video_details['items'][0]['snippet']['thumbnails']['default'].get('url'),\n",
    "                    'url': 'https://www.youtube.com/watch?v={}'.format(video_id)\n",
    "                }\n",
    "                data.append(video_data)\n",
    "            df = pd.DataFrame(data)\n",
    "            df_list.append(df)\n",
    "        except HttpError as e:\n",
    "            print(f'An HTTP error {e.resp.status} occurred:\\n{e.content}')\n",
    "        except Exception as e:\n",
    "            print(f'An error occurred:\\n{str(e)}')\n",
    "    df_concatenated = pd.concat(df_list, axis=0)\n",
    "    df_concatenated.to_csv(VIDEOS_FILE, mode='w', index=False)\n",
    "    return df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94d7761-5a07-4c44-a7ed-c4b6302aa0ac",
   "metadata": {},
   "source": [
    "### F. Retrieve comments for a list of videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b2e63bd-8072-4275-b854-2055d0fb10eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_comments():\n",
    "    videos = pd.read_csv(VIDEOS_FILE)\n",
    "    video_ids = videos['video_id'].tolist()\n",
    "    df_list =[]    \n",
    "    # Loop through all the video IDs and retrieve the comments\n",
    "    for video_id in video_ids:\n",
    "        print(f\"-> collecting comments for video: {video_id}\")\n",
    "        comments_list = []\n",
    "        try:\n",
    "            response = youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                textFormat='plainText'\n",
    "            ).execute()\n",
    "\n",
    "            # Loop through all the comments and extract the relevant information\n",
    "            for item in response['items']:\n",
    "                comment_id = item['snippet']['topLevelComment']['id']\n",
    "                comment_text = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "                comment_author = item['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "                comment_date = item['snippet']['topLevelComment']['snippet']['publishedAt']\n",
    "                like_count = item['snippet']['topLevelComment']['snippet']['likeCount']\n",
    "                reply_count = item['snippet']['totalReplyCount']\n",
    "                comments_list.append([video_id, comment_id, comment_text, comment_author, comment_date, like_count, None])\n",
    "                \n",
    "                if reply_count > 0:\n",
    "                    # Retrieve the replies to the top-level comment\n",
    "                    reply_response = youtube.comments().list(\n",
    "                        part='snippet',\n",
    "                        parentId=comment_id,\n",
    "                        textFormat='plainText'\n",
    "                    ).execute()\n",
    "                    \n",
    "                    # Loop through all the replies and extract the relevant information\n",
    "                    for reply_item in reply_response['items']:\n",
    "                        reply_id = reply_item['id']\n",
    "                        reply_text = reply_item['snippet']['textDisplay']\n",
    "                        reply_author = reply_item['snippet']['authorDisplayName']\n",
    "                        reply_date = reply_item['snippet']['publishedAt']\n",
    "                        reply_like_count = reply_item['snippet']['likeCount']\n",
    "                        comments_list.append([video_id, reply_id, reply_text, reply_author, reply_date, reply_like_count, comment_id])\n",
    "\n",
    "        except HttpError as error:\n",
    "            if error.resp.status == 403:\n",
    "                print(f'Comments are disabled for video ID {video_id}. Skipping...')\n",
    "            else:\n",
    "                raise error\n",
    "        \n",
    "        df = pd.DataFrame(comments_list, columns=['video_id', 'comment_id', 'comment_text', 'comment_author', 'comment_date', 'comment_like_count', 'parent_comment_id'])\n",
    "        df_list.append(df)\n",
    "    df_concatenated = pd.concat(df_list, axis=0)\n",
    "    df_concatenated.to_csv(COMMENTS_FILE, mode='w', index=False)\n",
    "    return df_concatenated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53640a66-446e-470c-9b7d-928367dcfee3",
   "metadata": {},
   "source": [
    "### G. Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4282dba5-9feb-4860-be39-efac070d9827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(VIDEOS_FILE, COMMENTS_FILE, stopwords):\n",
    "    # Load videos data\n",
    "    videos = pd.read_csv(VIDEOS_FILE)\n",
    "\n",
    "    # Clean videos data\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x) # remove punctuation\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x) # remove digits\n",
    "    videos['video_title'] = videos['video_title'].apply(lambda x: x.lower() if isinstance(x, str) else x) # convert to lowercase\n",
    "\n",
    "    # Save cleaned videos data to new CSV file, replacing the existing file\n",
    "    videos.to_csv(VIDEOS_FILE, index=False)\n",
    "\n",
    "    # Load comments data\n",
    "    comments = pd.read_csv(COMMENTS_FILE)\n",
    "\n",
    "    # Clean comments data\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x) if isinstance(x, str) else x)  # remove punctuation\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: re.sub(r'\\d+', '', x) if isinstance(x, str) else x)  # remove digits\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: x.lower() if isinstance(x, str) else x)  # convert to lowercase\n",
    "\n",
    "    # Remove duplicates\n",
    "    comments = comments.drop_duplicates()\n",
    "\n",
    "    # Remove rows with missing comment_text\n",
    "    comments = comments.dropna(subset=['comment_text'])\n",
    "\n",
    "    # Filter out comments that are not in English\n",
    "    try:\n",
    "        comments = comments[comments['comment_text'].apply(lambda x: langdetect.detect(x) == 'en')]\n",
    "    except langdetect.LangDetectException as e:\n",
    "        print(f\"non-english comment skipped ... {e}\")\n",
    "    # Stopword removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    comments['comment_text'] = comments['comment_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Save cleaned comments data to new CSV file, replacing the existing file\n",
    "    comments.to_csv(COMMENTS_FILE, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e72618-4248-458a-b753-bded1ce6f0ff",
   "metadata": {},
   "source": [
    "## H. Call the functions ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466fb70c-e28c-4b70-8a0b-a74f6fcf2bc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> collecting videos for channel: Fox News\n",
      "-> collecting videos for channel: BBC News\n",
      "-> collecting videos for channel: CNN\n",
      "-> collecting videos for channel: MSNBC\n",
      "-> Videos have been Collected ---------------------------\n",
      "-> collecting comments for video: d1eEWihvwpQ\n",
      "-> collecting comments for video: 0pkwjVhq230\n",
      "-> collecting comments for video: zwX5z_BvaN4\n",
      "-> collecting comments for video: 6G_-qe0iMBs\n",
      "-> collecting comments for video: xhpqX1NbusQ\n",
      "-> collecting comments for video: lJcjFGt9eQM\n",
      "-> collecting comments for video: bf4WRdPnf4I\n",
      "-> collecting comments for video: 2j3YtPVBT30\n",
      "-> collecting comments for video: 76J7SqZXG1g\n",
      "-> collecting comments for video: hIpCSt6NUJg\n",
      "-> collecting comments for video: gkR0jipzPRA\n",
      "-> collecting comments for video: gsp691RCGRU\n",
      "-> collecting comments for video: XkUsC7oO_bc\n",
      "-> collecting comments for video: xtW8F1Y3mAw\n",
      "-> collecting comments for video: felnRnj_zZ8\n",
      "-> collecting comments for video: Pevfw17JS6k\n",
      "-> collecting comments for video: 8506miurvpQ\n",
      "-> collecting comments for video: yA1hBTpXvb4\n",
      "-> collecting comments for video: ebH3NDpXoVs\n",
      "-> collecting comments for video: hpKJ1XSLjY8\n",
      "-> collecting comments for video: 9LBUutWsfBc\n",
      "-> collecting comments for video: WYlwQeE06og\n",
      "-> collecting comments for video: -r6mUnerPtc\n",
      "-> collecting comments for video: 2gBEJOZANDA\n",
      "-> collecting comments for video: DXBIQbJbZVw\n",
      "-> collecting comments for video: ZxICAFZY_io\n",
      "-> collecting comments for video: rLezK6SQNNk\n",
      "-> collecting comments for video: H9N9M9IjXD0\n",
      "-> collecting comments for video: ocLK8aEXR98\n",
      "-> collecting comments for video: 3ebpS88BAwA\n",
      "-> collecting comments for video: _FUiPbzoGSQ\n",
      "-> collecting comments for video: zFWn5IVLYQk\n",
      "-> collecting comments for video: LmNOVFozyjo\n",
      "-> collecting comments for video: J0Kavkyn46U\n",
      "-> collecting comments for video: EE1iX5HKELA\n",
      "-> collecting comments for video: Oei9_SRfaSw\n",
      "-> collecting comments for video: DZLQRA5XwPY\n",
      "-> collecting comments for video: ZOfdGTvBs8A\n",
      "-> collecting comments for video: tTcM7KseaC0\n",
      "-> collecting comments for video: ob5_NvJz1io\n",
      "-> collecting comments for video: -r_C-QsWEak\n",
      "-> collecting comments for video: fVpbMvz2auY\n",
      "-> collecting comments for video: jTspYIVHFRs\n",
      "-> collecting comments for video: jXHikITwlng\n",
      "-> collecting comments for video: 0sDAuWZPR-4\n",
      "-> collecting comments for video: Pw9p0w_hCNg\n",
      "-> collecting comments for video: EZ2TPymcdL4\n",
      "-> collecting comments for video: ebtpY4jZ9ds\n",
      "-> collecting comments for video: rqExfCNvDa0\n",
      "-> collecting comments for video: B6MURuLUetE\n",
      "-> collecting comments for video: YK_gq3PC7TE\n",
      "-> collecting comments for video: ArFQdvF8vDE\n",
      "-> collecting comments for video: gtf9j06EOvo\n",
      "-> collecting comments for video: 6CajGH3Q5RU\n",
      "Comments are disabled for video ID 6CajGH3Q5RU. Skipping...\n",
      "-> collecting comments for video: CuneZ1LhbQ0\n",
      "-> collecting comments for video: gXvSUiR2u2I\n",
      "-> collecting comments for video: dc11uZ7_PnU\n",
      "-> collecting comments for video: RHFV0XcJxww\n",
      "-> collecting comments for video: 3AgxeIVrH24\n",
      "-> collecting comments for video: kdUf0DhY6Ow\n",
      "-> collecting comments for video: xqK4NfOOF9A\n",
      "-> collecting comments for video: WE9gvJ8ou7g\n",
      "-> collecting comments for video: T-1-mlAGu1Y\n",
      "Comments are disabled for video ID T-1-mlAGu1Y. Skipping...\n",
      "-> collecting comments for video: UZ2YVvXL7Hk\n",
      "-> collecting comments for video: oKX_E7adVwg\n",
      "-> collecting comments for video: 1UtPZxGRZ5k\n",
      "-> collecting comments for video: K25NdiU0Gio\n",
      "-> collecting comments for video: alo1kLQXaf4\n",
      "-> collecting comments for video: zoZlDUsdtAg\n",
      "-> collecting comments for video: SLhGsTuE0ik\n",
      "-> collecting comments for video: bvQW8YkBVhQ\n",
      "-> collecting comments for video: r5yw9S3X7m4\n",
      "-> collecting comments for video: A2kiXc5XEdU\n",
      "-> collecting comments for video: P4SYSRp00uE\n",
      "-> collecting comments for video: Fihj2_qSDhU\n",
      "-> collecting comments for video: STgRHkB-vx4\n",
      "-> collecting comments for video: rrug35nWddk\n",
      "-> collecting comments for video: MnhnoM8X-YU\n",
      "-> collecting comments for video: P2y1sIr913Y\n",
      "-> collecting comments for video: glm0tMRHTkQ\n",
      "-> collecting comments for video: j-9CgwB6mus\n",
      "-> collecting comments for video: RSZoDTvgEr8\n",
      "-> collecting comments for video: W3ypPfbTyZs\n",
      "-> collecting comments for video: 2bSvL87r1Go\n",
      "-> collecting comments for video: SchJH53b3nk\n",
      "-> collecting comments for video: W2rIY9TREVw\n",
      "-> collecting comments for video: UNX6VuUKhTw\n",
      "Comments are disabled for video ID UNX6VuUKhTw. Skipping...\n",
      "-> collecting comments for video: aQHn2h-B594\n",
      "-> collecting comments for video: hURUDrx33TU\n",
      "-> collecting comments for video: v5fRX5V7mgE\n",
      "Comments are disabled for video ID v5fRX5V7mgE. Skipping...\n",
      "-> collecting comments for video: 3-b4NwMyRx8\n",
      "-> collecting comments for video: NwPwj45CAIc\n",
      "-> collecting comments for video: u-bZtzlVJvA\n",
      "-> collecting comments for video: PB7cAnZdKHA\n",
      "-> collecting comments for video: NbRnHkHnnkQ\n",
      "-> collecting comments for video: BbJVVpQ-RYM\n",
      "Comments are disabled for video ID BbJVVpQ-RYM. Skipping...\n",
      "-> collecting comments for video: V9ErfYHi_MU\n",
      "-> collecting comments for video: OX_i9tL2GcQ\n",
      "Comments are disabled for video ID OX_i9tL2GcQ. Skipping...\n",
      "-> collecting comments for video: BvOPt_gzI-Q\n",
      "-> collecting comments for video: FRtZA4WY8Mo\n",
      "-> collecting comments for video: Dvz20hLJ15I\n",
      "-> collecting comments for video: ibCuZkan4JM\n",
      "-> collecting comments for video: iVo5j9mZq7g\n",
      "-> collecting comments for video: OV1Vuv9zfzI\n",
      "-> collecting comments for video: VQqFr7_Y5NU\n",
      "-> collecting comments for video: sXpIrDGXeTg\n",
      "-> collecting comments for video: On7v-B28ZKk\n",
      "-> collecting comments for video: 3mfSSi-J7kI\n",
      "-> collecting comments for video: SfWtBPbE9so\n",
      "-> collecting comments for video: fGo5WqNjrOc\n",
      "-> collecting comments for video: 91CBoU0GqBM\n",
      "-> collecting comments for video: 0csWfr1uvj8\n",
      "-> collecting comments for video: 7lvZNcB0ZAs\n",
      "-> collecting comments for video: B4RRsV8nqrc\n",
      "-> collecting comments for video: GNDygQHBD60\n",
      "-> collecting comments for video: _waeIUyZGKA\n",
      "-> collecting comments for video: 4PunLFH-aY0\n",
      "-> collecting comments for video: SdqlNNAEju0\n",
      "-> collecting comments for video: 3AzIgAa0Cm8\n",
      "-> collecting comments for video: 5RunFCEZh_4\n",
      "-> collecting comments for video: PlNImCVwf4Q\n",
      "-> collecting comments for video: uRjiq4PSvvo\n",
      "-> collecting comments for video: o783-zkcX8Q\n",
      "-> collecting comments for video: iO0sa-dgVng\n",
      "-> collecting comments for video: nt06wMyE1zU\n",
      "-> collecting comments for video: fJBcVtzBwIc\n",
      "-> collecting comments for video: DuTy8aHiIjU\n",
      "-> collecting comments for video: wPM8v1z2xtg\n",
      "-> collecting comments for video: YoCfs-9AiiQ\n",
      "-> collecting comments for video: gl24Ts6JjZ8\n",
      "-> collecting comments for video: t8Nm3uXdxL8\n",
      "-> collecting comments for video: 02Wi5GSkdNk\n",
      "-> collecting comments for video: CEFo5skGRSI\n",
      "-> collecting comments for video: GEARtTBHXEw\n",
      "-> collecting comments for video: dXp0dhykNpo\n",
      "-> collecting comments for video: Q0I8fJa9wY0\n",
      "-> collecting comments for video: ZtkSYY9Z2fE\n",
      "-> collecting comments for video: 7cOExWfYqr0\n",
      "-> collecting comments for video: jvDOhfjP6uc\n",
      "-> collecting comments for video: 63M9ZQd2B7M\n",
      "-> collecting comments for video: RO09JTw-5kw\n",
      "-> collecting comments for video: tseMCjLqsQ0\n",
      "-> collecting comments for video: mKKy5cwc8GI\n",
      "-> collecting comments for video: w7ZRNNRWrNE\n",
      "-> collecting comments for video: wQrGiDQXbrk\n",
      "-> collecting comments for video: EEdfBb9ghTU\n",
      "-> collecting comments for video: hqdBLh7IXY0\n",
      "-> collecting comments for video: QQEQH59p17U\n",
      "-> collecting comments for video: uo43AbYlK2I\n",
      "-> collecting comments for video: bDBRxJTOKxM\n",
      "-> collecting comments for video: 1QPs-aN0c34\n",
      "-> collecting comments for video: sBmQam6kY0Q\n",
      "-> collecting comments for video: cMcCyGORMcE\n",
      "-> collecting comments for video: QwGfY5CJJZ0\n",
      "-> collecting comments for video: Tm_KWLzEhNE\n",
      "-> collecting comments for video: orWhiVs8Kjg\n",
      "-> collecting comments for video: 6H-hTDA7BPY\n",
      "-> collecting comments for video: 2gq5FV4rEgQ\n",
      "-> collecting comments for video: JZlZaCdXOCI\n",
      "-> collecting comments for video: 7jsU1XFuDtw\n",
      "-> collecting comments for video: 2WgNLO67Z18\n",
      "-> collecting comments for video: FFPBePaQKow\n",
      "-> collecting comments for video: W6NLnZnYSLs\n",
      "-> collecting comments for video: 1t8y0JrKKQI\n",
      "-> collecting comments for video: joat8uBUHFw\n",
      "-> collecting comments for video: qlxsHz1YfMA\n",
      "-> collecting comments for video: XREBr0Q1-34\n",
      "-> collecting comments for video: nhcYyVWWeR0\n",
      "-> collecting comments for video: unN8leCvHyM\n",
      "-> collecting comments for video: kyydqnYPa8o\n",
      "-> collecting comments for video: WMyzxpdUUFg\n",
      "-> collecting comments for video: FbOde3RKYZ8\n",
      "-> collecting comments for video: cCYOqx8qho8\n",
      "-> collecting comments for video: LtY1GVfaK9I\n",
      "-> collecting comments for video: cbbrlKl4Fak\n",
      "-> collecting comments for video: YMXjl8i_T5c\n",
      "-> collecting comments for video: 3L-kybxj5js\n",
      "-> collecting comments for video: 7T6iH0OJ_Sk\n",
      "-> collecting comments for video: TrwHT3W6MQE\n",
      "-> collecting comments for video: 4OWhhcB0oQA\n",
      "-> collecting comments for video: cOzHF_tHHr4\n",
      "-> collecting comments for video: RwUi63zcIBo\n",
      "-> collecting comments for video: DZ8P5eybEHE\n",
      "-> collecting comments for video: tSDrVpLQPTQ\n",
      "-> collecting comments for video: onqiLzlr4Yc\n",
      "-> collecting comments for video: TZuLAyZlams\n",
      "-> collecting comments for video: wPLkdevhWD0\n",
      "-> collecting comments for video: CeQhCHHvqPw\n",
      "-> collecting comments for video: 0GylbOHOujQ\n",
      "-> collecting comments for video: Mmcbwsf_6G0\n",
      "-> collecting comments for video: PU24xsnfMvA\n",
      "-> collecting comments for video: uhiEQ67NhZU\n",
      "-> collecting comments for video: ToQorCuhjDM\n",
      "-> collecting comments for video: JVZBz_cTAuo\n",
      "-> collecting comments for video: VZfYsPMn5ow\n",
      "-> collecting comments for video: c8TltWFwKuA\n",
      "-> collecting comments for video: -2h2bQYO-0E\n",
      "-> collecting comments for video: KtzNIX3fal8\n",
      "-> collecting comments for video: TySmb2_kZj0\n",
      "-> collecting comments for video: szORuX9KVc4\n",
      "-> Comments have been Collected -------------------------\n",
      "non-english comment skipped ... No features in text.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/isakoswald/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:84\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/isakoswald/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33m-> Comments have been Collected -------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# -------------- Clean the data -----------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mclean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVIDEOS_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOMMENTS_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m (\u001b[33m\"\u001b[39m\u001b[33m-> Data Cleaning has been Completed ---------------------\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mclean_data\u001b[39m\u001b[34m(VIDEOS_FILE, COMMENTS_FILE, stopwords)\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnon-english comment skipped ... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Stopword removal\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m stop_words = \u001b[38;5;28mset\u001b[39m(\u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     34\u001b[39m comments[\u001b[33m'\u001b[39m\u001b[33mcomment_text\u001b[39m\u001b[33m'\u001b[39m] = comments[\u001b[33m'\u001b[39m\u001b[33mcomment_text\u001b[39m\u001b[33m'\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x.split() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]))\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Save cleaned comments data to new CSV file, replacing the existing file\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:120\u001b[39m, in \u001b[36mLazyCorpusLoader.__getattr__\u001b[39m\u001b[34m(self, attr)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attr == \u001b[33m\"\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLazyCorpusLoader object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[33m__bases__\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:86\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     84\u001b[39m             root = nltk.data.find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.subdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[32m     89\u001b[39m corpus = \u001b[38;5;28mself\u001b[39m.__reader_cls(root, *\u001b[38;5;28mself\u001b[39m.__args, **\u001b[38;5;28mself\u001b[39m.__kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/corpus/util.py:81\u001b[39m, in \u001b[36mLazyCorpusLoader.__load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         root = \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/isakoswald/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.13/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Read the developer_key, service_name, and service_version from credentials.ini \n",
    "credentials = load_credentials()\n",
    "\n",
    "# Build a youtube object using the build function\n",
    "youtube = build(credentials['service_name'], credentials['service_version'],developerKey=credentials['developer_key'])\n",
    "\n",
    "# Exctract the video categories\n",
    "response = youtube.videoCategories().list(part='snippet', regionCode='UK').execute()\n",
    "VIDEO_CATEGORIES = {category['id']: category['snippet']['title'] for category in response['items']}\n",
    "\n",
    "#The following line shows how to extract the channel_id and channel_title of a video with video_id \"OOrW82pHlMQ\"\n",
    "# channel_id, channel_title = get_channel_info('OOrW82pHlMQ', youtube)\n",
    "# print(f'{channel_id}, {channel_title}')\n",
    "\n",
    "#  ------------  Get the data -------------------------------------\n",
    "get_channel_videos(list(CHANNELS.keys()), START_DATE, END_DATE, QUERY, VIDEO_CATEGORIES, max_videos = MAX_VIDEOS).head()\n",
    "print (\"-> Videos have been Collected ---------------------------\")\n",
    "get_videos_comments().head()\n",
    "print (\"-> Comments have been Collected -------------------------\")\n",
    "# -------------- Clean the data -----------------------------------\n",
    "clean_data(VIDEOS_FILE, COMMENTS_FILE, stopwords)\n",
    "print (\"-> Data Cleaning has been Completed ---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b656c2c-c1b0-4ebf-a228-b3b7e4634957",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
